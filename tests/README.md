# ğŸ§ª Sistema de Testing para Qtrazer

Este directorio contiene la suite completa de testing para el proyecto Qtrazer, implementando diferentes tipos de pruebas para asegurar la calidad y robustez del sistema.

## ğŸ“ Estructura del Sistema de Testing

```
tests/
â”œâ”€â”€ __init__.py                 # Paquete principal de testing
â”œâ”€â”€ conftest.py                 # ConfiguraciÃ³n y fixtures comunes
â”œâ”€â”€ unit/                       # Tests unitarios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api_client.py      # Tests del cliente API
â”‚   â”œâ”€â”€ test_database.py        # Tests del gestor de BD
â”‚   â”œâ”€â”€ test_update_model.py    # Tests del modelo de actualizaciÃ³n
â”‚   â””â”€â”€ test_controllers.py     # Tests de los controladores
â”œâ”€â”€ integration/                # Tests de integraciÃ³n
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_api_database.py    # Tests de integraciÃ³n API-BD
â”œâ”€â”€ functional/                 # Tests funcionales
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_user_scenarios.py  # Tests de escenarios de usuario
â”œâ”€â”€ performance/                # Tests de rendimiento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_load.py            # Tests de carga y rendimiento
â””â”€â”€ fixtures/                   # Fixtures de testing
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_data.py            # Datos de prueba especÃ­ficos
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Instalar Dependencias de Testing

```bash
pip install -r requirements-testing.txt
```

### 2. ConfiguraciÃ³n de pytest

El archivo `pytest.ini` ya estÃ¡ configurado con:
- Marcadores para diferentes tipos de tests
- ConfiguraciÃ³n de cobertura de cÃ³digo
- Filtros de warnings
- ConfiguraciÃ³n de reportes

### 3. Variables de Entorno (Opcional)

```bash
# Para tests de integraciÃ³n con base de datos real
export QTRAZER_TEST_DB_HOST=localhost
export QTRAZER_TEST_DB_PORT=5432
export QTRAZER_TEST_DB_NAME=test_siniestros
export QTRAZER_TEST_DB_USER=test_user
export QTRAZER_TEST_DB_PASSWORD=test_password
```

## ğŸ§ª EjecuciÃ³n de Tests

### Ejecutar Todos los Tests

```bash
pytest
```

### Ejecutar Tests por CategorÃ­a

```bash
# Tests unitarios
pytest -m unit

# Tests de integraciÃ³n
pytest -m integration

# Tests funcionales
pytest -m functional

# Tests de rendimiento
pytest -m performance
```

### Ejecutar Tests EspecÃ­ficos

```bash
# Tests de una clase especÃ­fica
pytest tests/unit/test_api_client.py::TestClienteAPI

# Tests de un mÃ©todo especÃ­fico
pytest tests/unit/test_database.py::TestGestorBaseDatos::test_conectar_exitoso

# Tests que contengan una palabra especÃ­fica
pytest -k "conexion"
```

### Ejecutar Tests con Cobertura

```bash
# Cobertura bÃ¡sica
pytest --cov=src

# Cobertura con reporte HTML
pytest --cov=src --cov-report=html

# Cobertura con reporte detallado
pytest --cov=src --cov-report=term-missing
```

### Ejecutar Tests en Paralelo

```bash
# Ejecutar tests en 4 procesos paralelos
pytest -n 4

# Ejecutar tests en modo auto (detecta nÃºmero de CPUs)
pytest -n auto
```

## ğŸ“Š Tipos de Tests Implementados

### 1. Tests Unitarios (`tests/unit/`)

**PropÃ³sito**: Probar componentes individuales de forma aislada.

**CaracterÃ­sticas**:
- Uso extensivo de mocks para aislar dependencias
- Tests rÃ¡pidos y determinÃ­sticos
- Cobertura completa de lÃ³gica de negocio

**Archivos**:
- `test_api_client.py`: Cliente API con mocks de requests
- `test_database.py`: Gestor de BD con mocks de psycopg2
- `test_update_model.py`: Modelo de actualizaciÃ³n
- `test_controllers.py`: Controladores principales

### 2. Tests de IntegraciÃ³n (`tests/integration/`)

**PropÃ³sito**: Probar la interacciÃ³n entre componentes.

**CaracterÃ­sticas**:
- Prueban flujos completos de datos
- Simulan integraciones reales
- Verifican consistencia entre componentes

**Archivos**:
- `test_api_database.py`: Flujo completo API â†’ BD

### 3. Tests Funcionales (`tests/functional/`)

**PropÃ³sito**: Probar escenarios de usuario reales.

**CaracterÃ­sticas**:
- Simulan casos de uso completos
- Prueban la funcionalidad end-to-end
- Verifican comportamiento del sistema

**Archivos**:
- `test_user_scenarios.py`: Escenarios de consulta y actualizaciÃ³n

### 4. Tests de Rendimiento (`tests/performance/`)

**PropÃ³sito**: Verificar el rendimiento del sistema.

**CaracterÃ­sticas**:
- Prueban con grandes volÃºmenes de datos
- Verifican tiempos de respuesta
- Monitorean uso de memoria

**Archivos**:
- `test_load.py`: Tests de carga y rendimiento

## ğŸ”§ Fixtures y Datos de Prueba

### Fixtures Comunes (`conftest.py`)

- `mock_database_connection`: ConexiÃ³n simulada a BD
- `mock_api_response`: Respuestas simuladas de API
- `sample_accident_data`: Datos de muestra de accidentes
- `mock_settings`: ConfiguraciÃ³n simulada

### Fixtures EspecÃ­ficos (`fixtures/test_data.py`)

- `datos_accidentes_completos`: Datos completos de accidentes
- `datos_actor_vial`: Datos de actores viales
- `datos_vehiculos`: Datos de vehÃ­culos
- `datos_causas`: Datos de causas de accidentes
- `datos_vias`: Datos de caracterÃ­sticas de vÃ­as

## ğŸ“ˆ Cobertura de CÃ³digo

### ConfiguraciÃ³n de Cobertura

El sistema estÃ¡ configurado para generar reportes de cobertura que incluyen:

- **Cobertura por archivo**: Muestra quÃ© archivos estÃ¡n probados
- **Cobertura por lÃ­nea**: Identifica lÃ­neas no cubiertas
- **Reporte HTML**: Interfaz visual para anÃ¡lisis de cobertura

### Ejecutar Cobertura

```bash
# Cobertura bÃ¡sica
pytest --cov=src

# Cobertura con reporte HTML (se genera en htmlcov/)
pytest --cov=src --cov-report=html

# Cobertura con reporte XML (para CI/CD)
pytest --cov=src --cov-report=xml
```

## ğŸš¨ Manejo de Errores en Tests

### Tests de Manejo de Errores

Los tests incluyen verificaciÃ³n de:

- **Errores de conexiÃ³n**: Timeouts, conexiones fallidas
- **Errores de API**: Respuestas invÃ¡lidas, errores HTTP
- **Errores de base de datos**: Fallos de consulta, rollbacks
- **Errores de validaciÃ³n**: Datos invÃ¡lidos, campos faltantes

### Ejemplos de Tests de Error

```python
def test_conectar_error_conexion(self, mock_connect):
    """Prueba el manejo de errores de conexiÃ³n."""
    mock_connect.side_effect = psycopg2.OperationalError("Connection failed")
    
    gestor = GestorBaseDatos()
    with pytest.raises(Exception, match="FallÃ³ la conexiÃ³n a la base de datos"):
        gestor.conectar()
```

## ğŸ”„ Tests de Concurrencia

### Tests de Threading

Los tests verifican el comportamiento correcto con:

- **MÃºltiples consultas simultÃ¡neas**
- **Actualizaciones concurrentes**
- **Manejo de estados compartidos**
- **Limpieza de recursos**

### Ejemplo de Test de Concurrencia

```python
def test_rendimiento_concurrencia_multiples_consultas(self):
    """Prueba el rendimiento con mÃºltiples consultas simultÃ¡neas."""
    # Ejecutar 10 consultas en paralelo
    threads = []
    for i in range(10):
        thread = threading.Thread(target=ejecutar_consulta, args=(i,))
        threads.append(thread)
        thread.start()
    
    # Verificar que todas se completaron
    for thread in threads:
        thread.join()
```

## ğŸ“Š Reportes y AnÃ¡lisis

### Generar Reportes HTML

```bash
pytest --cov=src --cov-report=html --html=reports/test_report.html
```

### Reportes Disponibles

- **Cobertura de cÃ³digo**: `htmlcov/index.html`
- **Reporte de tests**: `reports/test_report.html`
- **Reporte de cobertura XML**: Para integraciÃ³n con CI/CD

## ğŸš€ IntegraciÃ³n Continua (CI/CD)

### GitHub Actions

El sistema estÃ¡ preparado para integraciÃ³n con GitHub Actions:

```yaml
- name: Run Tests
  run: |
    pip install -r requirements-testing.txt
    pytest --cov=src --cov-report=xml --cov-report=term-missing
```

### Criterios de AprobaciÃ³n

- **Todos los tests deben pasar**
- **Cobertura mÃ­nima del 80%**
- **Sin warnings crÃ­ticos**
- **Tests de rendimiento dentro de lÃ­mites**

## ğŸ› SoluciÃ³n de Problemas

### Problemas Comunes

1. **ImportError**: Verificar que el PYTHONPATH incluya el directorio raÃ­z
2. **Mock no funciona**: Verificar que se estÃ© importando correctamente
3. **Tests lentos**: Usar `pytest -x` para parar en el primer fallo
4. **Cobertura baja**: Ejecutar `pytest --cov=src --cov-report=term-missing`

### Debug de Tests

```bash
# Ejecutar con output detallado
pytest -v -s

# Ejecutar un test especÃ­fico con debug
pytest -v -s tests/unit/test_database.py::TestGestorBaseDatos::test_conectar_exitoso

# Ejecutar con pdb en caso de fallo
pytest --pdb
```

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n de pytest

- [pytest Documentation](https://docs.pytest.org/)
- [pytest-cov Documentation](https://pytest-cov.readthedocs.io/)
- [pytest-mock Documentation](https://pytest-mock.readthedocs.io/)

### Mejores PrÃ¡cticas

- **Nombres descriptivos**: Los tests deben explicar quÃ© prueban
- **Arrange-Act-Assert**: Estructura clara de los tests
- **Fixtures reutilizables**: Evitar duplicaciÃ³n de cÃ³digo
- **Mocks apropiados**: Solo mockear dependencias externas

### ContribuciÃ³n

Para agregar nuevos tests:

1. **Crear archivo de test** en el directorio apropiado
2. **Usar fixtures existentes** cuando sea posible
3. **Agregar marcadores** apropiados (`@pytest.mark.unit`)
4. **Documentar** el propÃ³sito del test
5. **Verificar cobertura** del nuevo cÃ³digo

---

**Â¡El sistema de testing estÃ¡ listo para asegurar la calidad de Qtrazer! ğŸ¯**
